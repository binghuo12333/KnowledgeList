# Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰ç³»åˆ—æ¨¡å‹ä½¿ç”¨æŒ‡å—

<div align="center">
  <img src="https://img.alicdn.com/imgextra/i4/O1CN01nqE6sX1zGbH9M6U6O_!!6000000006571-2-tps-200-200.png" width="120" alt="Qwen Logo">
  <p>é˜¿é‡Œäº‘é€šä¹‰åƒé—®å…¨ç³»åˆ—æœ€æ–°å¼€æºæ¨¡å‹éƒ¨ç½²ä¸ä½¿ç”¨æŒ‡å—</p>
  <a href="https://github.com/QwenLM/Qwen"><img src="https://img.shields.io/github/stars/QwenLM/Qwen?style=social"></a>
  <a href="https://modelscope.cn/organization/qwen"><img src="https://img.shields.io/badge/ModelScope-Qwen-ff69b4"></a>
  <a href="https://huggingface.co/collections/Qwen"><img src="https://img.shields.io/badge/HuggingFace-Qwen-yellow"></a>
</div>

## ğŸ”— å®˜æ–¹èµ„æº

| å¹³å° | é“¾æ¥ | æ ¸å¿ƒç”¨é€” |
|------|------|----------|
| é˜¿é‡Œäº‘ç™¾ç‚¼æ§åˆ¶å° | [https://bailian.console.aliyun.com/cn-beijing/?tab=model#/model-market/all](https://bailian.console.aliyun.com/cn-beijing/?tab=model#/model-market/all) | åœ¨çº¿éƒ¨ç½²ã€APIè°ƒç”¨ã€è°ƒè¯• |
| Qwen å®˜æ–¹ä»“åº“ | [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen) | æºç ã€éƒ¨ç½²è„šæœ¬ã€ç¤ºä¾‹ |
| Hugging Face æ¨¡å‹åº“ | [https://huggingface.co/collections/Qwen](https://huggingface.co/collections/Qwen) | æ¨¡å‹ä¸‹è½½ã€HFç”Ÿæ€é€‚é… |
| é­”æ­ç¤¾åŒº | [https://modelscope.cn/organization/qwen](https://modelscope.cn/organization/qwen) | ä¸­æ–‡ç”Ÿæ€ã€ä¸€é”®éƒ¨ç½² |

## ğŸ“š æ¨¡å‹åˆ†ç±»åŠä½¿ç”¨è¯´æ˜

### 1. é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆQwen-2 Chatï¼‰
> æœ€æ–°è¿­ä»£ç‰ˆæœ¬ï¼Œæ¨ç†é€Ÿåº¦æå‡30%ã€æ˜¾å­˜å ç”¨é™ä½15%ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡

#### ğŸ“‹ æœ€æ–°æ¨¡å‹åˆ—è¡¨
- Qwen-2-0.5B-Chatï¼ˆè¶…è½»é‡ç‰ˆï¼Œçº¯CPUè¿è¡Œï¼‰
- Qwen-2-1.5B-Chatï¼ˆè½»é‡ç‰ˆï¼Œä½æ˜¾å­˜æ˜¾å¡é€‚é…ï¼‰
- Qwen-2-7B-Chatï¼ˆä¸»æµç‰ˆï¼Œæ¶ˆè´¹çº§æ˜¾å¡é¦–é€‰ï¼‰
- Qwen-2-14B-Chatï¼ˆå¹³è¡¡ç‰ˆï¼Œæ€§èƒ½ä¸èµ„æºå…¼é¡¾ï¼‰
- Qwen-2-72B-Chatï¼ˆæ——èˆ°ç‰ˆï¼Œ128Kè¶…é•¿ä¸Šä¸‹æ–‡ï¼‰
- Qwen-2-110B-Chatï¼ˆè¶…å¤§ç‰ˆï¼Œä¼ä¸šçº§éƒ¨ç½²ï¼‰

#### ğŸš€ å¿«é€Ÿå¯åŠ¨
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install "fschat[model_worker,webui]" transformers torch accelerate sentencepiece protobuf

# 1. å‘½ä»¤è¡Œäº¤äº’ï¼ˆINT4é‡åŒ–ï¼Œä½æ˜¾å­˜ï¼‰
python -m fastchat.serve.cli \
  --model-path ./Qwen-2-7B-Chat \
  --load-4bit \
  --temperature 0.7 \
  --trust-remote-code

# 2. WebUIå¯è§†åŒ–æœåŠ¡ï¼ˆè®¿é—®ï¼šhttp://localhost:7860ï¼‰
python -m fastchat.serve.controller &
python -m fastchat.serve.model_worker --model-path ./Qwen-2-7B-Chat --device cuda --load-4bit &
python -m fastchat.serve.gradio_web_server

# 3. OpenAIå…¼å®¹APIæœåŠ¡ï¼ˆç«¯å£8000ï¼‰
python -m fastchat.serve.openai_api_server \
  --model-path ./Qwen-2-7B-Chat \
  --host 0.0.0.0 \
  --port 8000 \
  --load-4bit

### 2. å¤šæ¨¡æ€æ¨¡å‹ï¼ˆQwen-VL/Audio 2.0ï¼‰
<div align="left">
  <img src="https://img.shields.io/badge/Multimodal-VL/Audio%202.0-9cf" alt="Multimodal">
  <img src="https://img.shields.io/badge/Context-8K-important" alt="Context">
</div>

> **æ ¸å¿ƒç‰¹æ€§**
> - ğŸ–¼ï¸ Qwen-VL 2.0ï¼šæ”¯æŒ4Kåˆ†è¾¨ç‡å›¾ç‰‡ã€å¤šå›¾å¯¹æ¯”ã€å¤æ‚å›¾è¡¨/OCRåˆ†æ
> - ğŸ™ï¸ Qwen-Audio 2.0ï¼šå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«/åˆæˆã€è¯­éŸ³ç¿»è¯‘ã€éŸ³é¢‘ç†è§£
> - ï¿½èåˆç‰ˆï¼šå›¾æ–‡éŸ³å¤šæ¨¡æ€äº¤äº’ï¼Œè·¨æ¨¡æ€è¯­ä¹‰ç†è§£èƒ½åŠ›è¡Œä¸šé¢†å…ˆ

#### ğŸ“‹ æœ€æ–°æ¨¡å‹åˆ—è¡¨
| æ¨¡å‹åç§° | é€‚ç”¨åœºæ™¯ | æ ¸å¿ƒä¼˜åŠ¿ |
| :------- | :------- | :------- |
| `Qwen-VL-2-7B-Chat` | é€šç”¨å›¾æ–‡äº¤äº’ | è½»é‡é«˜æ•ˆï¼Œæ¶ˆè´¹çº§æ˜¾å¡å¯è¿è¡Œ |
| `Qwen-VL-2-14B-Chat` | å¤æ‚å›¾æ–‡åˆ†æ | é«˜ç²¾åº¦å›¾è¡¨è§£è¯»ã€å¤šå›¾æ¨ç† |
| `Qwen-Audio-2-7B-Chat` | è¯­éŸ³äº¤äº’ | ä½å»¶è¿Ÿè¯­éŸ³è¯†åˆ«ï¼Œæ”¯æŒ10+è¯­è¨€ |
| `Qwen-VL-Audio-2-7B-Chat` | å…¨æ¨¡æ€äº¤äº’ | å›¾æ–‡éŸ³ä¸€ä½“åŒ–ç†è§£ä¸ç”Ÿæˆ |

#### ğŸš€ å¿«é€Ÿå¯åŠ¨
##### ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…å¤šæ¨¡æ€ä¸“å±ä¾èµ–
pip install torchvision pillow soundfile librosa transformers accelerate opencv-python pydub

1. å‘½ä»¤è¡Œå›¾æ–‡å¯¹è¯
# Qwen-VL-2-7B-Chatï¼ˆ8bité‡åŒ–ï¼‰
python -m fastchat.serve.cli \
  --model-path ./Qwen-VL-2-7B-Chat \
  --load-8bit \
  --trust-remote-code \
  --image ./demo_images/chart.png  # æ›¿æ¢ä¸ºæœ¬åœ°å›¾ç‰‡è·¯å¾„

2. WebUI å¯è§†åŒ–æœåŠ¡ï¼ˆæ”¯æŒå›¾ç‰‡ / éŸ³é¢‘ä¸Šä¼ ï¼‰
# åå°å¯åŠ¨ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
nohup python -m fastchat.serve.controller > controller.log 2>&1 &
nohup python -m fastchat.serve.model_worker \
  --model-path ./Qwen-VL-Audio-2-7B-Chat \
  --device cuda \
  --load-4bit > model_worker.log 2>&1 &
nohup python -m fastchat.serve.gradio_web_server \
  --multimodal \
  --server-port 7860 > webui.log 2>&1 &

# è®¿é—®åœ°å€ï¼šhttp://localhost:7860

3. åµŒå…¥æ¨¡å‹ï¼ˆQwen-Embedding V2ï¼‰
<div align="left"><img src="https://img.shields.io/badge/Embedding-V2-9cf" alt="Embedding"><img src="https://img.shields.io/badge/Dimension-768/1024-important" alt="Dimension"><img src="https://img.shields.io/badge/Context-8K-success" alt="Context"></div>
æ ¸å¿ƒç‰¹æ€§
ğŸŒ ä¸­è‹±æ–‡åŒè¯­åµŒå…¥ï¼Œè¯­ä¹‰å¯¹é½æ•ˆæœä¼˜äºä¸»æµå¼€æºæ¨¡å‹
ğŸ“œ é•¿æ–‡æœ¬åˆ†æ®µåµŒå…¥ï¼Œæ”¯æŒ 8K æ–‡æœ¬é•¿åº¦
âš¡ æ¨ç†é€Ÿåº¦æå‡ 50%ï¼Œé€‚é…é«˜å¹¶å‘æ£€ç´¢åœºæ™¯
ğŸ¯ æ•°å­¦ / ä»£ç ä¸“ç”¨ç‰ˆï¼Œå‚ç›´é¢†åŸŸæ•ˆæœä¼˜åŒ–

ğŸš€ å¿«é€Ÿä½¿ç”¨
åŸºç¡€åµŒå…¥ç”Ÿæˆ
pythonè¿è¡Œ
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Qwen-Embedding-V2 æ–‡æœ¬åµŒå…¥ç¤ºä¾‹"""
from transformers import AutoModel, AutoTokenizer
import torch

# ç¯å¢ƒé…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

# åŠ è½½æ¨¡å‹ï¼ˆå»ºè®®æå‰ä¸‹è½½åˆ°æœ¬åœ°ï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    "./Qwen-Embedding-V2",
    trust_remote_code=True,
    cache_dir="./cache"
)
model = AutoModel.from_pretrained(
    "./Qwen-Embedding-V2",
    trust_remote_code=True,
    torch_dtype=TORCH_DTYPE,
    cache_dir="./cache"
).to(DEVICE).eval()

# æ–‡æœ¬åµŒå…¥ç”Ÿæˆ
def get_text_embedding(texts: list) -> torch.Tensor:
    """
    ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
    :param texts: æ–‡æœ¬åˆ—è¡¨
    :return: å½’ä¸€åŒ–åçš„åµŒå…¥å‘é‡ [batch_size, dim]
    """
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=8192,
        return_tensors="pt"
    ).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # å–<bos> tokenä½œä¸ºå¥å­åµŒå…¥
        embeddings = outputs.last_hidden_state[:, 0]
        # å‘é‡å½’ä¸€åŒ–ï¼ˆæ£€ç´¢åœºæ™¯å¿…åšï¼‰
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
    
    return embeddings

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    test_texts = [
        "é€šä¹‰åƒé—®æ˜¯é˜¿é‡Œäº‘æ¨å‡ºçš„å¼€æºå¤§è¯­è¨€æ¨¡å‹",
        "Qwen-2 is the latest open-source LLM by Alibaba Cloud",
        "Qwen-Embedding-V2 æ”¯æŒé•¿æ–‡æœ¬è¯­ä¹‰å‘é‡ç”Ÿæˆ"
    ]
    
    embeddings = get_text_embedding(test_texts)
    print(f"åµŒå…¥å‘é‡ç»´åº¦ï¼š{embeddings.shape}")  # torch.Size([3, 1024])
    print(f"ç¬¬ä¸€æ¡æ–‡æœ¬å‘é‡å‰5ç»´ï¼š{embeddings[0][:5].cpu().numpy()}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    sim = torch.cosine_similarity(embeddings[0], embeddings[1], dim=0)
    print(f"ä¸­è‹±æ–‡è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š{sim.item():.4f}")

æ‰¹é‡åµŒå…¥ç”Ÿæˆï¼ˆç”Ÿäº§çº§ï¼‰pythonè¿è¡Œ
# æ‰¹é‡å¤„ç†ï¼ˆé¿å…OOMï¼‰
def batch_get_embeddings(texts: list, batch_size: int = 32) -> list:
    embeddings_list = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = get_text_embedding(batch_texts)
        embeddings_list.append(batch_embeddings.cpu())
    return torch.cat(embeddings_list, dim=0)

# ä½¿ç”¨ç¤ºä¾‹
# large_text_list = ["æ–‡æœ¬1", "æ–‡æœ¬2", ..., "æ–‡æœ¬10000"]
# all_embeddings = batch_get_embeddings(large_text_list, batch_size=32)

4. ä»£ç æ¨¡å‹ï¼ˆQwen-Coder 2ï¼‰
<div align="left"><img src="https://img.shields.io/badge/Coder-2.x-9cf" alt="Coder"><img src="https://img.shields.io/badge/Languages-20%2B-important" alt="Languages"><img src="https://img.shields.io/badge/Context-8K-success" alt="Context"></div>
æ ¸å¿ƒç‰¹æ€§
ğŸ’» æ”¯æŒ Python/Java/C++/Go/JavaScript ç­‰ 20 + ç¼–ç¨‹è¯­è¨€
ğŸ”§ ä»£ç ç”Ÿæˆ / è¡¥å…¨ / è°ƒè¯• / é‡æ„ / å•å…ƒæµ‹è¯•ç”Ÿæˆ
ğŸ“ ä»£ç è§£é‡Š / æ€§èƒ½ä¼˜åŒ– / é”™è¯¯ä¿®å¤
ğŸ¯ ç¼–ç¨‹é¢˜è§£ç­”ï¼Œæ”¯æŒ ACM/OJ æ ¼å¼

ğŸš€ å¿«é€Ÿå¯åŠ¨
1. å‘½ä»¤è¡Œä»£ç å¯¹è¯â€”â€”bashè¿è¡Œ
# Qwen-Coder-2-7B-Chatï¼ˆ4bité‡åŒ–ï¼‰
python -m fastchat.serve.cli \
  --model-path ./Qwen-Coder-2-7B-Chat \
  --load-4bit \
  --trust-remote-code \
  --prompt-template qwen_coder \
  --temperature 0.2 \
  --max-new-tokens 2048
2. API æœåŠ¡éƒ¨ç½²ï¼ˆä»£ç è¡¥å…¨åœºæ™¯ï¼‰â€”â€”bashè¿è¡Œ
# å¯åŠ¨OpenAIå…¼å®¹API
python -m fastchat.serve.openai_api_server \
  --model-path ./Qwen-Coder-2-7B-Chat \
  --host 0.0.0.0 \
  --port 8000 \
  --load-4bit \
  --trust-remote-code

# ä»£ç è¡¥å…¨è°ƒç”¨ç¤ºä¾‹ï¼ˆcurlï¼‰
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen-Coder-2-7B-Chat",
    "prompt": "def quick_sort(arr):\n    # å¿«é€Ÿæ’åºå®ç°",
    "max_tokens": 512,
    "temperature": 0.1,
    "stop": ["\ndef", "\nclass"]
  }'
3. ä»£ç è°ƒè¯•ä¸“ç”¨å¯åŠ¨â€”â€”bashè¿è¡Œ
python -m fastchat.serve.cli \
  --model-path ./Qwen-Coder-Debug-7B \
  --load-4bit \
  --trust-remote-code \
  --temperature 0.0

5. é‡æ’åºæ¨¡å‹ï¼ˆQwen-Rerankï¼‰
<div align="left"><img src="https://img.shields.io/badge/Rerank-M3-9cf" alt="Rerank"><img src="https://img.shields.io/badge/Latency-<10ms-important" alt="Latency"><img src="https://img.shields.io/badge/Context-512-success" alt="Context"></div>
æ ¸å¿ƒç‰¹æ€§
ğŸ” æ£€ç´¢é—®ç­”ï¼ˆRAGï¼‰åœºæ™¯ä¸“ç”¨ï¼Œæå‡æ£€ç´¢å‡†ç¡®ç‡ 30%+
ğŸ“š å¤šç²’åº¦æ–‡æœ¬é‡æ’ï¼Œæ”¯æŒçŸ­å¥ / é•¿æ–‡æœ¬ / è·¨è¯­è¨€é‡æ’
âš¡ è½»é‡çº§æ¨¡å‹ï¼Œå•æ¡æ¨ç†è€—æ—¶ < 10ms
ğŸ¯ é€‚é… ES/FAISS/PGVector ç­‰æ£€ç´¢å¼•æ“

ğŸš€ å¿«é€Ÿä½¿ç”¨
åŸºç¡€é‡æ’åºç¤ºä¾‹â€”â€”pythonè¿è¡Œ
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Qwen-Rerank-M3 æ£€ç´¢é‡æ’åºç¤ºä¾‹"""
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
from typing import List, Tuple

# ç¯å¢ƒé…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(
    "./Qwen-Rerank-M3",
    trust_remote_code=True,
    cache_dir="./cache"
)
model = AutoModelForSequenceClassification.from_pretrained(
    "./Qwen-Rerank-M3",
    trust_remote_code=True,
    torch_dtype=TORCH_DTYPE,
    cache_dir="./cache"
).to(DEVICE).eval()

# é‡æ’åºæ ¸å¿ƒå‡½æ•°
def rerank_documents(
    query: str,
    candidates: List[str],
    top_k: int = 5
) -> List[Tuple[str, float]]:
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    :param query: æŸ¥è¯¢è¯­å¥
    :param candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
    :param top_k: è¿”å›TOP-Kç»“æœ
    :return: æ’åºåçš„(æ–‡æ¡£, åˆ†æ•°)åˆ—è¡¨
    """
    # æ„é€ query-candidateå¯¹
    pairs = [[query, cand] for cand in candidates]
    
    # æ–‡æœ¬ç¼–ç 
    inputs = tokenizer(
        pairs,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(DEVICE)
    
    # é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°
    with torch.no_grad():
        scores = model(**inputs).logits.squeeze(-1).tolist()
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    ranked_pairs = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
    
    # è¿”å›TOP-K
    return ranked_pairs[:top_k]

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    # æ£€ç´¢åœºæ™¯ç¤ºä¾‹
    query = "å¦‚ä½•åœ¨æœ¬åœ°éƒ¨ç½²Qwen-2-7B-Chatå¹¶å¼€å¯APIæœåŠ¡"
    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
    retrieved_docs = [
        "Qwen-2æ¨¡å‹å¯é€šè¿‡FastChatéƒ¨ç½²OpenAIå…¼å®¹APIï¼Œç«¯å£å¯è‡ªå®šä¹‰",
        "Qwen-Embedding-V2ç”¨äºæ–‡æœ¬å‘é‡ç”Ÿæˆï¼Œé€‚é…RAGåœºæ™¯",
        "éƒ¨ç½²Qwen-2-7B-Chatéœ€è¦å®‰è£…torchã€transformersç­‰ä¾èµ–ï¼Œæ”¯æŒ4bité‡åŒ–",
        "Qwen-Coder-2å¯ç”ŸæˆPythonä»£ç ï¼Œæ”¯æŒä»£ç è°ƒè¯•åŠŸèƒ½",
        "FastChatæ”¯æŒå¤šæ¨¡å‹éƒ¨ç½²ï¼ŒåŒ…æ‹¬Qwenã€Llamaã€ChatGLMç­‰"
    ]
    
    # é‡æ’åº
    ranked_results = rerank_documents(query, retrieved_docs, top_k=3)
    
    # è¾“å‡ºç»“æœ
    print(f"æŸ¥è¯¢ï¼š{query}\n")
    print("é‡æ’åºç»“æœï¼ˆç›¸å…³æ€§ä»é«˜åˆ°ä½ï¼‰ï¼š")
    for idx, (doc, score) in enumerate(ranked_results, 1):
        print(f"{idx}. å¾—åˆ†ï¼š{score:.4f}")
        print(f"   æ–‡æœ¬ï¼š{doc}\n")

RAG é›†æˆç¤ºä¾‹ï¼ˆç”Ÿäº§çº§ï¼‰â€”â€”pythonè¿è¡Œ
# ä¸æ£€ç´¢å¼•æ“é›†æˆç¤ºä¾‹
def rag_pipeline(query: str, top_k: int = 3) -> str:
    """
    RAGå®Œæ•´æµç¨‹ï¼šæ£€ç´¢ -> é‡æ’åº -> ç”Ÿæˆ
    """
    # 1. ç¬¬ä¸€æ­¥ï¼šä»æ£€ç´¢å¼•æ“è·å–å€™é€‰æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿï¼‰
    retrieved_docs = retrieve_documents(query, top_k=10)
    
    # 2. ç¬¬äºŒæ­¥ï¼šé‡æ’åº
    ranked_docs = rerank_documents(query, retrieved_docs, top_k=top_k)
    
    # 3. ç¬¬ä¸‰æ­¥ï¼šæ„é€ promptå¹¶è°ƒç”¨LLMç”Ÿæˆå›ç­”
    context = "\n".join([doc for doc, _ in ranked_docs])
    prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š
{context}

é—®é¢˜ï¼š{query}
å›ç­”ï¼š"""
    
    # è°ƒç”¨Qwen-2ç”Ÿæˆå›ç­”
    response = generate_answer(prompt)
    return response


# DeepSeekï¼ˆæ·±åº¦æ±‚ç´¢ï¼‰ç³»åˆ—æ¨¡å‹ä½¿ç”¨æŒ‡å—

<div align="center">
  <img src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/assets/deepseek-logo.png" width="180" alt="DeepSeek Logo">
  <p align="center"><strong>æ·±åº¦æ±‚ç´¢å…¨ç³»åˆ—å¼€æºæ¨¡å‹éƒ¨ç½²ä¸ä½¿ç”¨æŒ‡å—</strong></p>
  
  <!-- å¾½ç« é›†åˆ -->
  <a href="https://github.com/deepseek-ai/DeepSeek-LLM" target="_blank">
    <img src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-LLM?style=social" alt="GitHub Stars">
  </a>
  <a href="https://huggingface.co/deepseek-ai" target="_blank">
    <img src="https://img.shields.io/badge/HuggingFace-DeepSeek-yellow" alt="HuggingFace">
  </a>
  <a href="https://modelscope.cn/organization/deepseek-ai" target="_blank">
    <img src="https://img.shields.io/badge/ModelScope-DeepSeek-ff69b4" alt="ModelScope">
  </a>
  <a href="https://www.deepseek.com/" target="_blank">
    <img src="https://img.shields.io/badge/å®˜ç½‘-DeepSeek-blue" alt="DeepSeek Official">
  </a>
  <a href="https://opensource.org/licenses/MIT" target="_blank">
    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  </a>
</div>

---

## ğŸ”— å®˜æ–¹èµ„æº

| å¹³å° | é“¾æ¥ | æ ¸å¿ƒç”¨é€” |
| :--- | :--- | :--- |
| å®˜æ–¹ GitHub | [https://github.com/deepseek-ai](https://github.com/deepseek-ai) | æºç ã€éƒ¨ç½²è„šæœ¬ã€ç¤ºä¾‹ä»£ç  |
| å®˜æ–¹ç½‘ç«™ | [https://www.deepseek.com/](https://www.deepseek.com/) | äº§å“ä»‹ç»ã€API æ–‡æ¡£ã€å•†ç”¨è¯´æ˜ |
| Hugging Face | [https://huggingface.co/deepseek-ai](https://huggingface.co/deepseek-ai) | æ¨¡å‹ä¸‹è½½ã€HF ç”Ÿæ€é€‚é… |
| é­”æ­ç¤¾åŒº | [https://modelscope.cn/organization/deepseek-ai](https://modelscope.cn/organization/deepseek-ai) | ä¸­æ–‡ç”Ÿæ€ã€ä¸€é”®éƒ¨ç½²ã€å›½å†…åŠ é€Ÿ |
| æŠ€æœ¯æ–‡æ¡£ | [https://docs.deepseek.com/](https://docs.deepseek.com/) | è¯¦ç»†éƒ¨ç½²æŒ‡å—ã€API ä½¿ç”¨è¯´æ˜ |

---

## ğŸ“š æ¨¡å‹åˆ†ç±»åŠä½¿ç”¨è¯´æ˜

### 1. é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆDeepSeek-LLMï¼‰
> **æ ¸å¿ƒç‰¹æ€§**ï¼šæ”¯æŒ 64K è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œä¸­æ–‡ç†è§£/æ¨ç†èƒ½åŠ›çªå‡ºï¼Œå¼€æº 7B/16B/33B ç‰ˆæœ¬ï¼Œå•†ç”¨å‹å¥½

#### ğŸ“‹ æœ€æ–°æ¨¡å‹åˆ—è¡¨
| æ¨¡å‹åç§° | è§„æ¨¡ | ä¸Šä¸‹æ–‡é•¿åº¦ | æ ¸å¿ƒä¼˜åŠ¿ | æ˜¾å­˜è¦æ±‚ï¼ˆINT4ï¼‰ |
| :------- | :--- | :--------- | :------- | :--------------- |
| `deepseek-llm-7b-chat` | 7B | 64K | è½»é‡é«˜æ•ˆï¼Œæ¶ˆè´¹çº§æ˜¾å¡å¯è¿è¡Œ | 6-8GB |
| `deepseek-llm-16b-chat` | 16B | 64K | å¹³è¡¡ç‰ˆï¼Œæ€§èƒ½ä¸èµ„æºå…¼é¡¾ | 12-14GB |
| `deepseek-llm-33b-chat` | 33B | 64K | æ——èˆ°ç‰ˆï¼Œæ¨ç†èƒ½åŠ›æ›´å¼º | 24-26GB |
| `deepseek-llm-7b-base` | 7B | 64K | åŸºç¡€ç‰ˆï¼Œé€‚åˆäºŒæ¬¡å¾®è°ƒ | 6-8GB |

#### ğŸš€ å¿«é€Ÿå¯åŠ¨
##### ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install "fschat[model_worker,webui]" transformers torch accelerate sentencepiece protobuf
```
# 1. å‘½ä»¤è¡Œäº¤äº’ï¼ˆINT4 é‡åŒ–ï¼‰
```
# DeepSeek-7B-Chat å‘½ä»¤è¡Œå¯¹è¯
python -m fastchat.serve.cli \
  --model-path ./deepseek-llm-7b-chat \
  --load-4bit \
  --trust-remote-code \
  --temperature 0.7 \
  --max-new-tokens 2048

# è¶…é•¿ä¸Šä¸‹æ–‡æµ‹è¯•ï¼ˆ64Kï¼‰
python -m fastchat.serve.cli \
  --model-path ./deepseek-llm-7b-chat \
  --load-4bit \
  --trust-remote-code \
  --max-context-length 65536 \
  --max-new-tokens 1024
```
# 2. WebUI å¯è§†åŒ–æœåŠ¡
```
# å¯åŠ¨æ§åˆ¶å™¨ï¼ˆåå°è¿è¡Œï¼‰
nohup python -m fastchat.serve.controller > controller.log 2>&1 &
# å¯åŠ¨æ¨¡å‹ Worker
nohup python -m fastchat.serve.model_worker \
  --model-path ./deepseek-llm-7b-chat \
  --device cuda \
  --load-4bit \
  --trust-remote-code > model_worker.log 2>&1 &

# å¯åŠ¨ WebUIï¼ˆè®¿é—®ï¼šhttp://localhost:7860ï¼‰
nohup python -m fastchat.serve.gradio_web_server > webui.log 2>&1 &
```
# 3. OpenAI å…¼å®¹ API æœåŠ¡
```
# å¯åŠ¨ API æœåŠ¡ï¼ˆç«¯å£ 8000ï¼‰
python -m fastchat.serve.openai_api_server \
  --model-path ./deepseek-llm-7b-chat \
  --host 0.0.0.0 \
  --port 8000 \
  --load-4bit \
  --trust-remote-code

# API è°ƒç”¨ç¤ºä¾‹ï¼ˆcurlï¼‰
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-llm-7b-chat",
    "messages": [{"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ DeepSeek å¤§æ¨¡å‹çš„æ ¸å¿ƒä¼˜åŠ¿"}]
  }'
```
### 2. å¤šæ¨¡æ€æ¨¡å‹ï¼ˆDeepSeek-VLï¼‰
ğŸš€ å¿«é€Ÿå¯åŠ¨
ç¯å¢ƒå‡†å¤‡
# å®‰è£…å¤šæ¨¡æ€ä¾èµ–
```
pip install torchvision pillow transformers accelerate opencv-python
```
# 1. å‘½ä»¤è¡Œå›¾æ–‡å¯¹è¯
```
# å›¾æ–‡é—®ç­”ï¼ˆæŒ‡å®šæœ¬åœ°å›¾ç‰‡ï¼‰
python -m fastchat.serve.cli \
  --model-path ./deepseek-vl-7b-chat \
  --load-4bit \
  --trust-remote-code \
  --image ./test_image.jpg \
  --temperature 0.7

# ç¤ºä¾‹æé—®ï¼š"åˆ†æè¿™å¼ å›¾è¡¨çš„æ•°æ®è¶‹åŠ¿ï¼Œå¹¶ç»™å‡ºç»“è®º"
```
# 2. WebUI å¤šæ¨¡æ€æœåŠ¡
```
# å¯åŠ¨æ§åˆ¶å™¨
python -m fastchat.serve.controller &

# å¯åŠ¨å¤šæ¨¡æ€ Worker
python -m fastchat.serve.model_worker \
  --model-path ./deepseek-vl-7b-chat \
  --device cuda \
  --load-4bit \
  --trust-remote-code &

# å¯åŠ¨å¸¦å›¾ç‰‡ä¸Šä¼ çš„ WebUI
python -m fastchat.serve.gradio_web_server --multimodal
```
### 3. åµŒå…¥æ¨¡å‹ï¼ˆDeepSeek-Embeddingï¼‰
æ ¸å¿ƒç‰¹æ€§ï¼šæ”¯æŒä¸­è‹±æ–‡åŒè¯­åµŒå…¥ï¼Œ64K é•¿æ–‡æœ¬åµŒå…¥ï¼Œé€‚é… RAG / æ£€ç´¢ / èšç±»åœºæ™¯ï¼Œå‘é‡ç»´åº¦ 1024
ğŸ“‹ æœ€æ–°æ¨¡å‹åˆ—è¡¨
| æ¨¡å‹åç§° | å‘é‡ç»´åº¦ | ä¸Šä¸‹æ–‡é•¿åº¦ | æ ¸å¿ƒä¼˜åŠ¿ | æ˜¾å­˜è¦æ±‚ï¼ˆINT4ï¼‰ |
| :------- | :--- | :--------- | :------- | :--------------- |
| `deepseek-embedding-v1` | 1024 | 64K | é€šç”¨æ–‡æœ¬åµŒå…¥ | â‰¤2GB |
| `deepseek-embedding-long-context` | 1024 | 64K | é•¿æ–‡æœ¬ä¸“ç”¨ | â‰¤3GB |

ğŸš€ å¿«é€Ÿä½¿ç”¨
```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""DeepSeek-Embedding æ–‡æœ¬åµŒå…¥ç¤ºä¾‹"""
from transformers import AutoModel, AutoTokenizer
import torch

# ç¯å¢ƒé…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(
    "./deepseek-embedding-v1",
    trust_remote_code=True
)
model = AutoModel.from_pretrained(
    "./deepseek-embedding-v1",
    trust_remote_code=True,
    torch_dtype=TORCH_DTYPE
).to(DEVICE).eval()

# ç”Ÿæˆæ–‡æœ¬åµŒå…¥
def get_embedding(texts: list) -> torch.Tensor:
    """
    ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰
    :param texts: æ–‡æœ¬åˆ—è¡¨
    :return: åµŒå…¥å‘é‡ [batch_size, 1024]
    """
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=65536,  # 64K ä¸Šä¸‹æ–‡
        return_tensors="pt"
    ).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # å– <bos> token ä½œä¸ºå¥å­åµŒå…¥
        embeddings = outputs.last_hidden_state[:, 0]
        # å‘é‡å½’ä¸€åŒ–ï¼ˆæ£€ç´¢åœºæ™¯å¿…åšï¼‰
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
    
    return embeddings

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    test_texts = [
        "DeepSeek å¤§æ¨¡å‹æ”¯æŒ 64K è¶…é•¿ä¸Šä¸‹æ–‡",
        "DeepSeek-Embedding é€‚é…æ£€ç´¢é—®ç­”åœºæ™¯",
        "DeepSeek is an open-source LLM with 64K context window"
    ]
    
    embeddings = get_embedding(test_texts)
    print(f"åµŒå…¥å‘é‡ç»´åº¦ï¼š{embeddings.shape}")  # torch.Size([3, 1024])
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    sim = torch.cosine_similarity(embeddings[0], embeddings[1], dim=0)
    print(f"æ–‡æœ¬ç›¸ä¼¼åº¦ï¼š{sim.item():.4f}")
```

### 4. ä»£ç æ¨¡å‹ï¼ˆDeepSeek-Coderï¼‰
æ ¸å¿ƒç‰¹æ€§ï¼šæ”¯æŒ 80+ ç¼–ç¨‹è¯­è¨€ï¼Œ128K ä»£ç ä¸Šä¸‹æ–‡ï¼Œä»£ç ç”Ÿæˆ / è¡¥å…¨ / è°ƒè¯• / é‡æ„ï¼Œé€‚é…ä¸“ä¸šå¼€å‘åœºæ™¯
ğŸ“‹ æœ€æ–°æ¨¡å‹åˆ—è¡¨
| æ¨¡å‹åç§° | è§„æ¨¡ | ä¸Šä¸‹æ–‡é•¿åº¦ | æ ¸å¿ƒä¼˜åŠ¿ | æ˜¾å­˜è¦æ±‚ï¼ˆINT4ï¼‰ |
| :------- | :--- | :--------- | :------- | :--------------- |
| `deepseek-coder-7b-instruct` | 7B | 128K | åŸºç¡€ä»£ç å¼€å‘ | 6-8GB |
| `deepseek-coder-16b-instruct` | 16B | 128K | å¤æ‚ä»£ç å¼€å‘ | 12-14GB |
| `deepseek-coder-33b-instruct` | 33B | 128K | æ——èˆ°ä»£ç æ¨¡å‹ | 24-26GB |
| `deepseek-coder-v2-7b` | 7B | 128K | ç¬¬äºŒä»£ä»£ç æ¨¡å‹ | 6-8GB |

ğŸš€ å¿«é€Ÿå¯åŠ¨
1. å‘½ä»¤è¡Œä»£ç å¯¹è¯
```
# ä»£ç å¼€å‘ï¼ˆINT4 é‡åŒ–ï¼‰
python -m fastchat.serve.cli \
  --model-path ./deepseek-coder-7b-instruct \
  --load-4bit \
  --trust-remote-code \
  --prompt-template deepseek_coder \
  --temperature 0.2 \
  --max-new-tokens 2048

2. ä»£ç è¡¥å…¨ API æœåŠ¡
# å¯åŠ¨ä»£ç è¡¥å…¨ API
python -m fastchat.serve.openai_api_server \
  --model-path ./deepseek-coder-7b-instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --load-4bit \
  --trust-remote-code

# ä»£ç è¡¥å…¨è°ƒç”¨ç¤ºä¾‹
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-coder-7b-instruct",
    "prompt": "def quick_sort(arr):\n    # å¿«é€Ÿæ’åºå®ç°",
    "max_tokens": 512,
    "temperature": 0.1
  }'
```

### 5. é‡æ’åºæ¨¡å‹ï¼ˆDeepSeek-Rerankï¼‰
æ ¸å¿ƒç‰¹æ€§ï¼šæ£€ç´¢é—®ç­”ï¼ˆRAGï¼‰ä¸“ç”¨ï¼Œæ”¯æŒä¸­è‹±æ–‡é‡æ’ã€é•¿æ–‡æœ¬é‡æ’ï¼Œå•æ¡æ¨ç†è€—æ—¶ <10ms
ğŸ“‹ æœ€æ–°æ¨¡å‹åˆ—è¡¨
| æ¨¡å‹åç§° | è§„æ¨¡ | ä¸Šä¸‹æ–‡é•¿åº¦ | æ ¸å¿ƒä¼˜åŠ¿ | æ˜¾å­˜è¦æ±‚ï¼ˆINT4ï¼‰ |
| :------- | :--- | :--------- | :------- | :--------------- |
| `deepseek-rerank-base` | 1.3B | 512 | é€šç”¨é‡æ’ | â‰¤2GB |
| `deepseek-rerank-large` | 2.6B | 1024 | é«˜ç²¾åº¦é‡æ’ | â‰¤4GB |
| `deepseek-rerank-long` | 2.6B | 2048 | é•¿æ–‡æœ¬é‡æ’ | â‰¤4GB |

ğŸš€ å¿«é€Ÿä½¿ç”¨
```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""DeepSeek-Rerank æ£€ç´¢é‡æ’åºç¤ºä¾‹"""
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
from typing import List, Tuple

# ç¯å¢ƒé…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(
    "./deepseek-rerank-base",
    trust_remote_code=True
)
model = AutoModelForSequenceClassification.from_pretrained(
    "./deepseek-rerank-base",
    trust_remote_code=True,
    torch_dtype=TORCH_DTYPE
).to(DEVICE).eval()

# é‡æ’åºæ ¸å¿ƒå‡½æ•°
def rerank_docs(
    query: str,
    candidates: List[str],
    top_k: int = 5
) -> List[Tuple[str, float]]:
    """
    æ£€ç´¢ç»“æœé‡æ’åº
    :param query: æŸ¥è¯¢è¯­å¥
    :param candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
    :param top_k: è¿”å› TOP-K ç»“æœ
    :return: (æ–‡æ¡£, åˆ†æ•°) åˆ—è¡¨
    """
    # æ„é€  query-candidate å¯¹
    pairs = [[query, doc] for doc in candidates]
    
    # æ–‡æœ¬ç¼–ç 
    inputs = tokenizer(
        pairs,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(DEVICE)
    
    # é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°
    with torch.no_grad():
        scores = model(**inputs).logits.squeeze(-1).tolist()
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
    return ranked[:top_k]

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    query = "å¦‚ä½•éƒ¨ç½² DeepSeek-7B-Chat å¹¶å¼€å¯ 64K ä¸Šä¸‹æ–‡"
    candidates = [
        "DeepSeek-LLM æ”¯æŒ 64K è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œéƒ¨ç½²æ—¶éœ€è®¾ç½® max_context_length",
        "DeepSeek-Embedding ç”¨äºæ–‡æœ¬å‘é‡ç”Ÿæˆï¼Œé€‚é… RAG åœºæ™¯",
        "éƒ¨ç½² DeepSeek-7B-Chat éœ€è¦å®‰è£… torch>=2.0.0 å’Œ transformers>=4.35.0",
        "DeepSeek-Coder æ”¯æŒ 128K ä»£ç ä¸Šä¸‹æ–‡ï¼Œé€‚åˆä¸“ä¸šå¼€å‘"
    ]
    
    # é‡æ’åº
    results = rerank_docs(query, candidates, top_k=3)
    print("é‡æ’åºç»“æœï¼ˆç›¸å…³æ€§ä»é«˜åˆ°ä½ï¼‰ï¼š")
    for idx, (doc, score) in enumerate(results, 1):
        print(f"{idx}. å¾—åˆ†ï¼š{score:.4f} | æ–‡æœ¬ï¼š{doc}")
```

ğŸ“ é‡è¦æ³¨æ„äº‹é¡¹
# 1. æ¨¡å‹ä¸‹è½½
```
# æ–¹æ³• 1ï¼šé­”æ­ä¸‹è½½ï¼ˆå›½å†…æ¨èï¼‰
pip install modelscope
modelscope download --model=deepseek-ai/deepseek-llm-7b-chat --local-dir=./deepseek-llm-7b-chat

# æ–¹æ³• 2ï¼šHugging Face ä¸‹è½½
pip install huggingface-hub
huggingface-cli download deepseek-ai/deepseek-llm-7b-chat --local-dir ./deepseek-llm-7b-chat
```
# 2. å…³é”®éƒ¨ç½²æŠ€å·§
è¶…é•¿ä¸Šä¸‹æ–‡ï¼šå¯åŠ¨æ—¶æ·»åŠ  --max-context-length 65536 å¼€å¯ 64K ä¸Šä¸‹æ–‡
é‡åŒ–å¯åŠ¨ï¼šä½æ˜¾å­˜åœºæ™¯å¿…åŠ  --load-4bit/--load-8bitï¼Œæ€§èƒ½æŸå¤± <5%
ä¾èµ–å…¼å®¹ï¼šå»ºè®®ä½¿ç”¨ torch>=2.0.0ã€transformers>=4.35.0
å•†ç”¨è¯´æ˜ï¼šDeepSeek ç³»åˆ—éµå¾ª MIT åè®®ï¼Œå¯å…è´¹å•†ç”¨ï¼ˆéœ€ä¿ç•™ç‰ˆæƒå£°æ˜ï¼‰
# 3. å¸¸è§é—®é¢˜
64K ä¸Šä¸‹æ–‡å¯åŠ¨æŠ¥é”™ï¼šéœ€å‡çº§ transformers åˆ° 4.35.0+
å¤šæ¨¡æ€æ¨¡å‹å›¾ç‰‡åŠ è½½å¤±è´¥ï¼šå®‰è£… pillow>=10.0.0ã€opencv-python>=4.8.0
é‡åŒ–å¯åŠ¨å¤±è´¥ï¼šå®‰è£… bitsandbytes>=0.41.0
